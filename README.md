# LLMs from scratch

A compact collection of simple, didactic implementations used to demonstrate Transformer building blocks (attention mechanisms, positional encodings, and the Transformer model).

## Index

- [**Attention Module**](./attention/README.md)
  - [Scaled Dot-Product Attention](./attention/scaled_dot_product_attention.py)
  - [Multi-Head Attention](./attention/multi_head_attention.py)
  - [Multi-Query Attention](./attention/multi_query_attention.py)
  - [Grouped Query Attention](./attention/grouped_query_attention.py)
  - [Multi-Head Latent Attention](./attention/multi_head_latent_attention.py)
  - [Flash Attention](./attention/flash_attention.py)
  - [Projection](./attention/projection.py)
  - [Layer Normalization](./attention/layernorm.py)

- [**Positional Encoding Module**](./positional_encoding/README.md)
  - [Sinusoidal Positional Encoding](./positional_encoding/sinusoidal.py)
  - [Rotary Positional Encoding (RoPE)](./positional_encoding/rotary.py)

- [**Transformers Module**](./transformers/README.md)
  - [Transformer](./transformers/transformer.py)
  - [Encoder](./transformers/encoder.py)
  - [Decoder](./transformers/decoder.py)

